{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/site-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n",
      "/usr/local/lib/python3.6/site-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import keras.backend\n",
    "import keras.callbacks\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.optimizers\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('SVG')\n",
    "\n",
    "import helper.callbacks\n",
    "import helper.model_builder\n",
    "import helper.visualize\n",
    "import helper.data_provider\n",
    "import helper.metrics\n",
    "\n",
    "import skimage.io\n",
    "import sklearn.metrics\n",
    "\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "const_lr = 1e-4\n",
    "\n",
    "tag = '01'\n",
    "\n",
    "# Output dirs\n",
    "out_dir = '/data1/image-segmentation/BBBC022/unet/experiments/' + tag + '/out'\n",
    "tb_log_dir = \"/data1/image-segmentation/BBBC022/unet/tensorboard/\" + tag + \"/\"\n",
    "chkpt_dir = \"/data1/image-segmentation/BBBC022/unet/experiments/\" + tag + \"/checkpoints/\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "os.makedirs(tb_log_dir, exist_ok=True)\n",
    "os.makedirs(chkpt_dir, exist_ok=True)\n",
    "\n",
    "# Files\n",
    "chkpt_file = chkpt_dir + \"{epoch:04d}.hdf5\"\n",
    "csv_log_file = \"/data1/image-segmentation/BBBC022/unet/experiments/\" + tag + \"/log.csv\"\n",
    "\n",
    "# Input dirs\n",
    "train_dir_x = '/data1/image-segmentation/BBBC022/unet/split/training/x/'\n",
    "train_dir_y = '/data1/image-segmentation/BBBC022/unet/split/training/y/'\n",
    "val_dir = \"/data1/image-segmentation/BBBC022/unet/split/validation/\"\n",
    "\n",
    "# Learning Settings\n",
    "rescale_labels = False\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "batch_size = 4\n",
    "steps_per_epoch = 2 * 4 / batch_size\n",
    "\n",
    "# make sure these number for to the validation set\n",
    "val_batch_size = 10\n",
    "val_steps = int(50 * 4 / val_batch_size)\n",
    "\n",
    "# generator only params\n",
    "dim1 = 256\n",
    "dim2 = 256\n",
    "\n",
    "bit_depth = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 1 classes.\n",
      "Found 50 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# build session running on GPU 1\n",
    "configuration = tf.ConfigProto()\n",
    "#configuration.gpu_options.allow_growth = True\n",
    "configuration.gpu_options.visible_device_list = \"\"\n",
    "session = tf.Session(config = configuration)\n",
    "\n",
    "# apply session\n",
    "keras.backend.set_session(session)\n",
    "\n",
    "train_gen = helper.data_provider.random_sample_generator(\n",
    "    train_dir_x,\n",
    "    train_dir_y,\n",
    "    batch_size,\n",
    "    bit_depth,\n",
    "    dim1,\n",
    "    dim2,\n",
    "    rescale_labels\n",
    ")\n",
    "\n",
    "val_gen = helper.data_provider.single_data_from_images(\n",
    "    val_dir + 'x/',\n",
    "     val_dir + 'y/',\n",
    "     val_batch_size,\n",
    "     bit_depth,\n",
    "     dim1,\n",
    "     dim2,\n",
    "     rescale_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  a = keras.layers.Convolution2D(64, 3, 3, **option_dict_conv)(x)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:27: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  a = keras.layers.BatchNormalization(**option_dict_bn)(a)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  a = keras.layers.Convolution2D(64, 3, 3, **option_dict_conv)(a)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:33: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  a = keras.layers.BatchNormalization(**option_dict_bn)(a)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:39: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  b = keras.layers.Convolution2D(128, 3, 3, **option_dict_conv)(y)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:41: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  b = keras.layers.BatchNormalization(**option_dict_bn)(b)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:45: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  b = keras.layers.Convolution2D(128, 3, 3, **option_dict_conv)(b)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:47: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  b = keras.layers.BatchNormalization(**option_dict_bn)(b)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:53: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  c = keras.layers.Convolution2D(256, 3, 3, **option_dict_conv)(y)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:55: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  c = keras.layers.BatchNormalization(**option_dict_bn)(c)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:59: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  c = keras.layers.Convolution2D(256, 3, 3, **option_dict_conv)(c)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:61: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  c = keras.layers.BatchNormalization(**option_dict_bn)(c)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:67: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  d = keras.layers.Convolution2D(512, 3, 3, **option_dict_conv)(y)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:69: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  d = keras.layers.BatchNormalization(**option_dict_bn)(d)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:73: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  d = keras.layers.Convolution2D(512, 3, 3, **option_dict_conv)(d)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:75: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  d = keras.layers.BatchNormalization(**option_dict_bn)(d)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:83: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  y = keras.layers.merge([d, c], concat_axis=3, mode=\"concat\")\n",
      "/usr/local/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:85: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  e = keras.layers.Convolution2D(256, 3, 3, **option_dict_conv)(y)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:87: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  e = keras.layers.BatchNormalization(**option_dict_bn)(e)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:91: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  e = keras.layers.Convolution2D(256, 3, 3, **option_dict_conv)(e)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:93: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  e = keras.layers.BatchNormalization(**option_dict_bn)(e)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:99: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  y = keras.layers.merge([e, b], concat_axis=3, mode=\"concat\")\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:101: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  f = keras.layers.Convolution2D(128, 3, 3, **option_dict_conv)(y)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:103: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  f = keras.layers.BatchNormalization(**option_dict_bn)(f)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:107: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  f = keras.layers.Convolution2D(128, 3, 3, **option_dict_conv)(f)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:109: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  f = keras.layers.BatchNormalization(**option_dict_bn)(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 256, 256, 1)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 256, 256, 64)  640         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 256, 256, 64)  256         conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 256, 256, 64)  36928       batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 256, 256, 64)  256         conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 128, 128, 64)  0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 128, 128, 128) 73856       max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 128, 128, 128) 512         conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 128, 128, 128) 147584      batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 128, 128, 128) 512         conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 64, 64, 128)   0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 64, 64, 256)   295168      max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 64, 64, 256)   1024        conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 64, 64, 256)   590080      batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 64, 64, 256)   1024        conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 32, 32, 256)   0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 32, 32, 512)   1180160     max_pooling2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 32, 32, 512)   2048        conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 32, 32, 512)   2359808     batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 32, 32, 512)   2048        conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)   (None, 64, 64, 512)   0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 64, 64, 768)   0           up_sampling2d_1[0][0]            \n",
      "                                                                   batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 64, 64, 256)   1769728     merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 64, 64, 256)   1024        conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 64, 64, 256)   590080      batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 64, 64, 256)   1024        conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)   (None, 128, 128, 256) 0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 128, 128, 384) 0           up_sampling2d_2[0][0]            \n",
      "                                                                   batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 128, 128, 128) 442496      merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 128, 128, 128) 512         conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 128, 128, 128) 147584      batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 128, 128, 128) 512         conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)   (None, 256, 256, 128) 0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 256, 256, 192) 0           up_sampling2d_3[0][0]            \n",
      "                                                                   batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 256, 256, 64)  110656      merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 256, 256, 64)  256         conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 256, 256, 64)  36928       batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 256, 256, 64)  256         conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 256, 256, 3)   195         batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 256, 256, 3)   0           conv2d_15[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 7,793,155\n",
      "Trainable params: 7,787,523\n",
      "Non-trainable params: 5,632\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:115: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  y = keras.layers.merge([f, a], concat_axis=3, mode=\"concat\")\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:119: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  y = keras.layers.Convolution2D(64, 3, 3, **option_dict_conv)(y)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:121: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  y = keras.layers.BatchNormalization(**option_dict_bn)(y)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:125: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  y = keras.layers.Convolution2D(64, 3, 3, **option_dict_conv)(y)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:127: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(momentum=0.9)`\n",
      "  y = keras.layers.BatchNormalization(**option_dict_bn)(y)\n",
      "/home/jccaicedo/unet4nuclei/unet4nuclei/helper/model_builder.py:138: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (1, 1), activation=\"relu\", padding=\"same\")`\n",
      "  y = keras.layers.Convolution2D(3, 1, 1, **option_dict_conv)(y)\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = helper.model_builder.get_model_3_class(dim1, dim2)\n",
    "model.summary()\n",
    "\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = [keras.metrics.categorical_accuracy, helper.metrics.recall, helper.metrics.precision]\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr = const_lr)\n",
    "\n",
    "model.compile(loss=loss, metrics=metrics, optimizer=optimizer)\n",
    "\n",
    "# CALLBACKS\n",
    "# save model after each epoch\n",
    "callback_model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=chkpt_file,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=False\n",
    ")\n",
    "callback_csv = keras.callbacks.CSVLogger(filename=csv_log_file)\n",
    "# callback_splits_and_merges = helper.callbacks.SplitsAndMergesLoggerBoundary(\n",
    "#     'images',\n",
    "#     val_gen,\n",
    "#     gen_calls = val_steps,\n",
    "#     log_dir=tb_log_dir\n",
    "# )\n",
    "\n",
    "callbacks=[callback_model_checkpoint, callback_csv] #, callback_splits_and_merges]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Found 100 images.\n",
      "Found 100 annotations.\n",
      "2/2 [==============================] - 131s - loss: 262.9672 - categorical_accuracy: 0.5700 - recall: 0.9998 - precision: 1.8817 - val_loss: 255.0504 - val_categorical_accuracy: 0.7558 - val_recall: 1.0000 - val_precision: 6553600000000.0000\n",
      "Epoch 2/2\n",
      "1/2 [==============>...............] - ETA: 7s - loss: 141.3766 - categorical_accuracy: 0.8868 - recall: 0.9998 - precision: 1.3797"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "statistics = model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "# visualize learning stats\n",
    "#helper.visualize.visualize_learning_stats_boundary_hard(statistics, out_dir, metrics)\n",
    "\n",
    "print('Done! :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
